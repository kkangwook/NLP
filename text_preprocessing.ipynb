{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f517a3-e5f9-4602-8f6f-e22960565443",
   "metadata": {},
   "source": [
    "# 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c449e-db57-4199-9d23-1a15bfa318ef",
   "metadata": {},
   "source": [
    "# 1. 토큰화-나누기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17032b12-f8c1-48d3-956b-186090d6906d",
   "metadata": {},
   "source": [
    "## 1-1. 단어 토큰화\n",
    "- 구두점( , . ! ? ' 등)이나 특수 문자를 단순 제외해서는 안 된다-> 의미를 가지고 있을수도 있으므로\n",
    "- 줄임말(I'm)이나 띄어쓰기(rock 'n' roll)단순 분리도 조심"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9107276-da2d-444f-a3e6-bc10519c8f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "114560fa-6330-4fa3-968b-5a463dc88126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'home-based', 'shop', '.']\n",
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'home', 'based', 'shop']\n"
     ]
    }
   ],
   "source": [
    "text=\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a home-based shop.\"\n",
    "\n",
    "print(word_tokenize(text))  #-> 구두점, 특수문자 대부분 분리\n",
    "print(text_to_word_sequence(text))  #-> .은 제거하는데 '는 유지, 전부 소문자화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310999b5-3568-4781-872d-5af66cdaffc4",
   "metadata": {},
   "source": [
    "## 1-2. 표준토큰화 예제\n",
    "- treebankwordtoeknizer사용\n",
    "- 규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.\n",
    "- 규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d2dafa-be20-4dff-93b1-f81830123d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb31daf5-6e96-4e5e-8579-f4587dbd74f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트리뱅크: ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'home-based', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer=TreebankWordTokenizer()  #객체 생성\n",
    "\n",
    "print('트리뱅크:',tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57800c5-07b2-4b7a-9599-b59963805c07",
   "metadata": {},
   "source": [
    "## 1-3. 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66062678-27a2-4928-8918-605f7c265f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print('문장 토큰화2 :',sent_tokenize(text))   #-> 단순 . 으로 구분X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51e38c-3daa-4e6c-9c19-26297052babc",
   "metadata": {},
   "source": [
    "# 2. 정제(Cleaning)와 정규화(Normalization)\n",
    "- 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터(불필요한)를 제거한다.\n",
    "- 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.(US, USA)\n",
    "- 대, 소문자 통합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c5c1b-4280-4b42-90b9-e0008852f89d",
   "metadata": {},
   "source": [
    "## 2-1. 불필요한 단어 제거\n",
    "- 등장 빈도가 적은 단어-> 필요할수도있지만 대량의 문서 분류시 도움이 안될 확률 큼\n",
    "- 길이가 짧은 단어-> 한국어의 경우 어려움 but 영어의 경우 1~2자 제거는 도움됨 ex) I, a, it, at, to, on, by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d298141a-83ce-4e7c-8711-02c2f950c342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' was wondering anyone out there could enlighten this car.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그냥 1~2자 제거하기\n",
    "import re\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "\n",
    "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')  # \\W는 단어가 아닌것, \\w:알파벳+숫자, \\b는 공백\n",
    "shortword.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc912cc3-36b2-4722-8495-1688ab45a01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'wondering', 'anyone', 'could', 'enlighten', 'car', '.']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 꺼내와 제거하기\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=stopwords.words('english')\n",
    "tokens=word_tokenize(text)\n",
    "result=[]\n",
    "\n",
    "for i in tokens:\n",
    "    if i not in stop_words:\n",
    "        result.append(i)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f5840fc-8845-4d3c-9cdd-9123e26cd356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['was', 'wondering', 'anyone', 'out', 'there', 'could', 'enlighten', 'this', 'car']\n",
      "['was', 'wondering', 'anyone', 'out', 'there', 'could', 'enlighten', 'this', 'car']\n",
      "['I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car.']\n"
     ]
    }
   ],
   "source": [
    "# 정규표현식으로 내가 지정해서 뽑아오거나 삭제\n",
    "import re\n",
    "standard=re.compile(r'\\w{3,}')\n",
    "print(standard.findall(text))     #3글자 이상인 애를 리스트로\n",
    "\n",
    "#nltk함수 사용\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "standard=RegexpTokenizer(r'\\w{3,}')    #3글자 이상만 가져옴\n",
    "print(standard.tokenize(text))\n",
    "\n",
    "standard=RegexpTokenizer(r'\\s+',gaps=True)  #빈칸을 기준으로 나눔\n",
    "print(standard.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f3537-b214-4813-8dbe-e2480bb942f6",
   "metadata": {},
   "source": [
    "## 2-2. 정규화\n",
    "- 어간 추출 (stem)->의미만 알아볼 수 있게 자름->사전에 없을수 있음\n",
    "- 표제어 추출 (lemmatization)-> 뿌리 단어로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2465f985-ac6a-4b80-b334-54f39fc2458a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "#어간추출\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "\n",
    "tokens=word_tokenize(sentence)\n",
    "stemmer=PorterStemmer()\n",
    "print([stemmer.stem(i) for i in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9eb70b1-caf3-4823-927d-ac3bc4a59d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 후 : ['This', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sounding', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n",
      "표제어 추출 후 : ['This', 'be', 'not', 'the', 'map', 'we', 'find', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'name', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'cross', 'and', 'the', 'write', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "# 표제어추출\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print('표제어 추출 후 :',[lemmatizer.lemmatize(i) for i in tokens])\n",
    "print('표제어 추출 후 :',[lemmatizer.lemmatize(i,'v') for i in tokens]) #이때 verb옵션 넣으면 found가 find로"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034fa7e-e118-4d8d-8aad-5ccbd92eabb8",
   "metadata": {},
   "source": [
    "# 3. 정수 인코딩(Integer Encoding)\n",
    "- 텍스트를 숫자로 바꿈\n",
    "- 빈도수가 높은 단어를 우선으로 낮은 숫자 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f4f8e41-aa00-41a1-83fe-f03351ba2b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret!\\nThe Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word.\\nHis barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. The barber went up a huge mountain.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터\n",
    "text = \"\"\"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret!\n",
    "The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word.\n",
    "His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. The barber went up a huge mountain.\"\"\"\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e4a237-7cd9-4e2e-9d13-edcccbbaeeac",
   "metadata": {},
   "source": [
    "### 1.문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f13e8b8c-35ca-4111-a678-834fb3720e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'The barber went up a huge mountain.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ff3b3-f6cd-4cb6-9119-f83fc4708599",
   "metadata": {},
   "source": [
    "### 2. 단어 토큰화 후 소문자화+불용어 제거+짧은단어 제거+각 단어 빈도수 세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed7bb269-355b-4f64-805e-9e6406290d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "vocab = {}    #고유 단어와 그 개수\n",
    "preprocessed_sentences = []    #전처리한 문장들\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for sentence in sentences:\n",
    "    # 단어 토큰화\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    result = []\n",
    "\n",
    "    for word in tokenized_sentence: \n",
    "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄인다.\n",
    "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거한다.\n",
    "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거한다.\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0 \n",
    "                vocab[word] += 1    # 특정 단어가 나올때마다 value값을 1씩 증가\n",
    "    preprocessed_sentences.append(result) \n",
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df31afdd-7b86-4d2f-ad20-2c427f34cf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 : {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "# 빈도수\n",
    "print('단어 집합 :',vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc2f4cf-4927-4e68-94f7-ef94274034cc",
   "metadata": {},
   "source": [
    "### 3. 빈도수대로 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a43d7f1c-de47-4c78-b275-c66a9347a628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "vocab_sorted = sorted(vocab.items(),key = lambda x:x[1], reverse = True)  #x[1]은 key,value값 중 value값을 기준으로 정렬하겠다\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0acf1a-28e7-4cab-b8e2-eaaa8fc43299",
   "metadata": {},
   "source": [
    "### 4. 높은 빈도수에 낮은 번호 부여(1부터)-이때 빈도수 1이하는 제거(이는 본인이 판다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99ba7705-a353-400d-8010-9063ce58f08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab_sorted :\n",
    "    if frequency > 1 : # 빈도수가 작은 단어는 제외.\n",
    "        i = i + 1\n",
    "        word_to_index[word] = i\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23afa60b-3a35-4835-8e7c-f25b6cb0bfc8",
   "metadata": {},
   "source": [
    "### 5. 본인의 판단하에 빈도수가 높은 상위 단어 n개 가져오기(나는 5개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa00db16-dba2-4a96-b925-b4ee471fbd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "\n",
    "# 인덱스가 5 초과인 단어 제거\n",
    "words_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1]\n",
    "\n",
    "# 해당 단어에 대한 인덱스 정보를 삭제\n",
    "for w in words_frequency:\n",
    "    del word_to_index[w]\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb26c85-d153-457c-8ea5-4b0a2f72bf69",
   "metadata": {},
   "source": [
    "### 6. OOV(out of vocabulary)문제\n",
    "- 일부만 가져오고 나머지는 삭제함에따라 인코딩 안된 토큰들이 존재\n",
    "- 이들에게는 전부 하나의 새로운 정수로 인코딩->나는 6으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a394432-8c3a-4be7-9494-35fbe465a11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n"
     ]
    }
   ],
   "source": [
    "#앞으로 인코딩 안되 애들에게는 6을 부여하겠다\n",
    "word_to_index['OOV'] = len(word_to_index) + 1   #6\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b304e5c0-02c8-4e03-8f67-7356d834118d",
   "metadata": {},
   "source": [
    "### 7. 각 문자와 인코딩 정수를 맵핑-인코딩 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4ca6b61-a915-44d6-a385-7c62ff42e8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = []    #결과\n",
    "for sentence in preprocessed_sentences:  #각 문장에서\n",
    "    encoded_sentence = []\n",
    "    for word in sentence:       #각 단어에서\n",
    "        try:\n",
    "            # 단어 집합에 있는 단어라면 해당 단어의 정수를 리턴.\n",
    "            encoded_sentence.append(word_to_index[word])\n",
    "        except KeyError:\n",
    "            # 만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴.\n",
    "            encoded_sentence.append(word_to_index['OOV'])\n",
    "    encoded_sentences.append(encoded_sentence)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e92d4b-9321-448d-8ceb-0e32b0c61c57",
   "metadata": {},
   "source": [
    "## 다른 방법들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941847d8-cf32-4215-83f3-c2fc7fd3e7a9",
   "metadata": {},
   "source": [
    "## 1. counter사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cf730da3-c834-47b4-bd01-c596e19cc61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3df26ea2-6456-4c8f-b2f7-f6d7002ee73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2132f17-463f-49fc-8a27-b4339ad3b31b",
   "metadata": {},
   "source": [
    "### 평탄화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "197bce35-8f5d-4c9b-86d0-4537562e41fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "in_one_list=sum(preprocessed_sentences,[])  #[]옵션을 집어넣음으로써 평탄화 가능\n",
    "print(in_one_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326dfd75-9436-47e5-b1be-d7a960d20dcf",
   "metadata": {},
   "source": [
    "### 빈도수세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "95b13492-6ec3-49a0-80c2-582ea459dcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter(in_one_list)\n",
    "print(vocab)   #딕셔너리 형태"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90b30d-0095-4032-b1a4-ee797a63a7f4",
   "metadata": {},
   "source": [
    "### 상위 빈도수 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d0f08674-3d32-4144-9182-d13d84f5af5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "top5 = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "top5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ca1d7-c90d-40a5-b91c-728ceda431a5",
   "metadata": {},
   "source": [
    "### 빈도수 높은애들부터 낮은 수 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12f12e3a-10c2-47cf-a053-4af99eecbfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in top5 :\n",
    "    i = i + 1\n",
    "    word_to_index[word] = i\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb36e75-9ac5-4138-89c3-08cfa3325f86",
   "metadata": {},
   "source": [
    "### 이후 동일 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c515314e-1ad1-4044-86cf-33a76b608627",
   "metadata": {},
   "source": [
    "## 2. NLTK의 FreqDist 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e76aab8e-8122-4555-a876-3a7117dba5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "# np.hstack으로 문장 구분을 제거\n",
    "vocab = FreqDist(np.hstack(preprocessed_sentences))\n",
    "\n",
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "print(vocab)\n",
    "\n",
    "# 정수 부여\n",
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}   #딕셔너리는 이런 형태로 컴프리헨션 가능!!!!!!!!\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1a6d78-af23-4eec-bb20-512be1d207e2",
   "metadata": {},
   "source": [
    "# 3. keras사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ecd91d29-1b0e-4258-9314-87c8fd1bcd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0b97cb0-8291-4226-b256-5c624ff35cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae56229-e661-4190-8bce-72d750c79fc4",
   "metadata": {},
   "source": [
    "## 자동으로 바로 빈도수대로 낮은 정수부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "516b1cb0-c1df-47bf-8c76-012e63530ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "# tokenizer() 빈칸도 가능->전부 출력\n",
    "tokenizer = Tokenizer(num_words = 5 + 1) #num_words로 상위만 가져오기-> 0부터 카운트하므로 5개 사용할려면 5+1=6을 넣어줌\n",
    "\n",
    "# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)   #정수부여함수\n",
    "\n",
    "print(tokenizer.word_counts)   #빈도수 보여줌 (전부보여주긴함)\n",
    "print(tokenizer.word_index)    #이 메서드로 부여된 정수들 보여줌(이때 전부 보여주긴함)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b22724-e32e-4184-8e49-92167d13088a",
   "metadata": {},
   "source": [
    "## 바로 text를 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "21b19d2b-4aeb-4ac0-8d06-6708a0751758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "# text_to_sequence사용\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))  #근데 이렇게 하면 oov는 그냥 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb81197-ab20-4f0f-9653-1ee600337c36",
   "metadata": {},
   "source": [
    "## 이때 oov를 넣고 싶다면 옵션 사용-> oov값은 6이 아닌 맨 처음값인 1이됨\n",
    "## 빈도수 제일 높은 단어는 2부터 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b4f956af-37b3-4809-a063-ae1ab2504e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5  #상위 5개\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')  # 예비용 하나 더 필요해서 +2, oov_token옵션과 해당 키 이름 지정->이 값이 1\n",
    "\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)  #정수 부여\n",
    "\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))  #가장 빈도수 높은 애는 2, oov는 1로 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248e365b-035d-44be-aac8-76c761cb232d",
   "metadata": {},
   "source": [
    "# 4. 패딩(padding)-길이 일치시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b1fa51f-226f-490a-a19d-a9d96cde3c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 6],\n",
       " [2, 1, 6],\n",
       " [2, 4, 6],\n",
       " [1, 3],\n",
       " [3, 5, 4, 3],\n",
       " [4, 3],\n",
       " [2, 5, 1],\n",
       " [2, 5, 1],\n",
       " [2, 5, 3],\n",
       " [1, 1, 4, 3, 1, 2, 1],\n",
       " [2, 1, 4, 1]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded=tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b61981a-fa31-404d-879b-fd78be0d0468",
   "metadata": {},
   "source": [
    "## 4-1-1. 가장 긴 길이 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "08267380-837c-47b2-abf8-bfb34e20cabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len=max(len(i) for i in encoded)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc802e-de8a-4406-b2dd-cc322834098c",
   "metadata": {},
   "source": [
    "## 4-1-2. 부족한 부분은 뒤에 0들로 채워넣음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cc853463-7e79-46de-b8b2-6e0b2fb59ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 6, 0, 0, 0, 0, 0],\n",
       "       [2, 1, 6, 0, 0, 0, 0],\n",
       "       [2, 4, 6, 0, 0, 0, 0],\n",
       "       [1, 3, 0, 0, 0, 0, 0],\n",
       "       [3, 5, 4, 3, 0, 0, 0],\n",
       "       [4, 3, 0, 0, 0, 0, 0],\n",
       "       [2, 5, 1, 0, 0, 0, 0],\n",
       "       [2, 5, 1, 0, 0, 0, 0],\n",
       "       [2, 5, 3, 0, 0, 0, 0],\n",
       "       [1, 1, 4, 3, 1, 2, 1],\n",
       "       [2, 1, 4, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in encoded:\n",
    "    while len(i) < max_len:\n",
    "        i.append(0)       #최대길이(7)가 될때까지 0 채워넣음\n",
    "\n",
    "padded_np = np.array(encoded)    #행렬화\n",
    "padded_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfba63e-bcb1-479c-bc36-7c47b8912884",
   "metadata": {},
   "source": [
    "## 4-2 keras사용하기 ->자동으로 최대길이 기준으로 0채워넣음\n",
    "- pad_sequence사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "39c3ff91-31ab-4bea-984b-e092693657bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798039fb-43e8-4bd9-86b9-a44f4fecebf1",
   "metadata": {},
   "source": [
    "### pad_sequence의 기본형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6477e7f9-b284-4206-b367-ab07b33e94a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 2, 6],\n",
       "       [0, 0, 0, 0, 2, 1, 6],\n",
       "       [0, 0, 0, 0, 2, 4, 6],\n",
       "       [0, 0, 0, 0, 0, 1, 3],\n",
       "       [0, 0, 0, 3, 5, 4, 3],\n",
       "       [0, 0, 0, 0, 0, 4, 3],\n",
       "       [0, 0, 0, 0, 2, 5, 1],\n",
       "       [0, 0, 0, 0, 2, 5, 1],\n",
       "       [0, 0, 0, 0, 2, 5, 3],\n",
       "       [1, 1, 4, 3, 1, 2, 1],\n",
       "       [0, 0, 0, 2, 1, 4, 1]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#앞에 0\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(encoded)\n",
    "padded   #앞을 0으로 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "65a73b83-246e-438a-8fdd-37cc80daa852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 6, 0, 0, 0, 0, 0],\n",
       "       [2, 1, 6, 0, 0, 0, 0],\n",
       "       [2, 4, 6, 0, 0, 0, 0],\n",
       "       [1, 3, 0, 0, 0, 0, 0],\n",
       "       [3, 5, 4, 3, 0, 0, 0],\n",
       "       [4, 3, 0, 0, 0, 0, 0],\n",
       "       [2, 5, 1, 0, 0, 0, 0],\n",
       "       [2, 5, 1, 0, 0, 0, 0],\n",
       "       [2, 5, 3, 0, 0, 0, 0],\n",
       "       [1, 1, 4, 3, 1, 2, 1],\n",
       "       [2, 1, 4, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뒤에 0채움\n",
    "padded = pad_sequences(encoded, padding='post')   #post옵션\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7121f79c-165a-4de5-b942-129afda94135",
   "metadata": {},
   "source": [
    "### 최대길이 직접 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e3ba197c-45e9-422b-8e99-8508eabfd21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 6, 0, 0, 0],\n",
       "       [2, 1, 6, 0, 0],\n",
       "       [2, 4, 6, 0, 0],\n",
       "       [1, 3, 0, 0, 0],\n",
       "       [3, 5, 4, 3, 0],\n",
       "       [4, 3, 0, 0, 0],\n",
       "       [2, 5, 1, 0, 0],\n",
       "       [2, 5, 1, 0, 0],\n",
       "       [2, 5, 3, 0, 0],\n",
       "       [4, 3, 1, 2, 1],\n",
       "       [2, 1, 4, 1, 0]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최대길이 직접 지정\n",
    "padded = pad_sequences(encoded, padding='post', maxlen=5)\n",
    "padded   #이때 앞에것들이 잘림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7ba81609-aff7-499c-b2d0-36d52b69239c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 6, 0, 0, 0],\n",
       "       [2, 1, 6, 0, 0],\n",
       "       [2, 4, 6, 0, 0],\n",
       "       [1, 3, 0, 0, 0],\n",
       "       [3, 5, 4, 3, 0],\n",
       "       [4, 3, 0, 0, 0],\n",
       "       [2, 5, 1, 0, 0],\n",
       "       [2, 5, 1, 0, 0],\n",
       "       [2, 5, 3, 0, 0],\n",
       "       [1, 1, 4, 3, 1],\n",
       "       [2, 1, 4, 1, 0]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뒤에것들이 잘리게 하기\n",
    "padded = pad_sequences(encoded, padding='post', truncating='post', maxlen=5)  #truncating을 post로\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d8e92-58bf-49fb-86a5-6f0fcb4d089e",
   "metadata": {},
   "source": [
    "### 채울 value를 직접 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cc9a23f6-5f70-467e-b8bd-15a22145602c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,   6, 100, 100, 100, 100, 100],\n",
       "       [  2,   1,   6, 100, 100, 100, 100],\n",
       "       [  2,   4,   6, 100, 100, 100, 100],\n",
       "       [  1,   3, 100, 100, 100, 100, 100],\n",
       "       [  3,   5,   4,   3, 100, 100, 100],\n",
       "       [  4,   3, 100, 100, 100, 100, 100],\n",
       "       [  2,   5,   1, 100, 100, 100, 100],\n",
       "       [  2,   5,   1, 100, 100, 100, 100],\n",
       "       [  2,   5,   3, 100, 100, 100, 100],\n",
       "       [  1,   1,   4,   3,   1,   2,   1],\n",
       "       [  2,   1,   4,   1, 100, 100, 100]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0말고 다른 숫자 넣기\n",
    "padded = pad_sequences(encoded, padding='post', value=100)  #value옵션 사용\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2f9890-7ce6-44eb-9b71-24e01ea693b6",
   "metadata": {},
   "source": [
    "# 5. one-hot encoding(정수인코딩의 다른 버전)\n",
    "- 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e734d7c-c986-4eef-a2fa-9af3944fee08",
   "metadata": {},
   "source": [
    "## 5-1. 빈도수에 따라 정수 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c0fa9516-6cd8-4ebe-a0f4-a2566c75c091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 : {'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])     #빈도수 높은 단어들에 정수 부여\n",
    "print('단어 집합 :',tokenizer.word_index)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd7e30d-e082-41b6-a295-5f8b53464ab6",
   "metadata": {},
   "source": [
    "## 5-2. 고유한 단어들로만 이루어진 text생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "60b6ff62-895b-4ed7-b5a2-6ec42e133e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 1, 6, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "# 고유단어들만 있는 sub_text\n",
    "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
    "encoded = tokenizer.texts_to_sequences([sub_text])[0]   #[[X]]형태여서 [0]해야 X나옴\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165fb157-2380-4fc6-a977-321a84fecf35",
   "metadata": {},
   "source": [
    "## 5-3. to_categorical로 ->각 단어별 정수값 위치에 1이고 나머지는 0인 행들이 단어개수만큼 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e1f83f23-70bf-462d-a717-6dec89bc2a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "one_hot = to_categorical(encoded)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf390ae-1e4a-4cbf-8f1e-93b67bb2f063",
   "metadata": {},
   "source": [
    "## 한계->단어수가 늘어날수록 벡터차원이 늘어나 비효율적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053cadee-47af-4b28-a95f-3bb5cef2564b",
   "metadata": {},
   "source": [
    "# 6. train용과 test용 데이터 분리\n",
    "- 훈련 데이터: x_train(시험지), y_train(정답)\n",
    "- 테스트 데이터: x_test(시험지), y_test(정답->실제로 모델의 예측률이 얼마인지 얘를 통해 측정)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771c286-7740-451f-b437-c9327e0f9a88",
   "metadata": {},
   "source": [
    "## 6-1. x(시험지)와 y(정답)분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4370af5f-a195-43d3-b54e-986553452f94",
   "metadata": {},
   "source": [
    "### zip함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "79002626-f518-4866-b9f1-ac1c22813a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 데이터 : ('a', 'b', 'c')\n",
      "y 데이터 : (1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "# 1. zip함수 사용\n",
    "sequences = [['a', 1], ['b', 2], ['c', 3]]\n",
    "x, y = zip(*sequences)   #*로 풀어주고 zip으로 x=a, y=1로\n",
    "print('X 데이터 :',x)\n",
    "print('y 데이터 :',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46c464-1084-45b0-a52c-2dba4764139e",
   "metadata": {},
   "source": [
    "### pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4f2fda9f-84da-4a06-a3c8-4d7339731af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>메일 본문</th>\n",
       "      <th>스팸 메일 유무</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>당신에게 드리는 마지막 혜택!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>내일 뵐 수 있을지 확인 부탁드...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>도연씨. 잘 지내시죠? 오랜만입...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(광고) AI로 주가를 예측할 수 있다!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    메일 본문  스팸 메일 유무\n",
       "0        당신에게 드리는 마지막 혜택!         1\n",
       "1    내일 뵐 수 있을지 확인 부탁드...         0\n",
       "2    도연씨. 잘 지내시죠? 오랜만입...         0\n",
       "3  (광고) AI로 주가를 예측할 수 있다!         1"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. pandas의 데이터프레임 사용\n",
    "import pandas as pd\n",
    "\n",
    "values = [['당신에게 드리는 마지막 혜택!', 1],\n",
    "['내일 뵐 수 있을지 확인 부탁드...', 0],\n",
    "['도연씨. 잘 지내시죠? 오랜만입...', 0],\n",
    "['(광고) AI로 주가를 예측할 수 있다!', 1]]\n",
    "columns = ['메일 본문', '스팸 메일 유무']\n",
    "\n",
    "df = pd.DataFrame(values, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "baf7cef3-9134-464b-adc8-976174b03898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          당신에게 드리는 마지막 혜택!\n",
      "1      내일 뵐 수 있을지 확인 부탁드...\n",
      "2      도연씨. 잘 지내시죠? 오랜만입...\n",
      "3    (광고) AI로 주가를 예측할 수 있다!\n",
      "Name: 메일 본문, dtype: object 0    1\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "Name: 스팸 메일 유무, dtype: int64\n",
      "----------------------------------------\n",
      "['당신에게 드리는 마지막 혜택!', '내일 뵐 수 있을지 확인 부탁드...', '도연씨. 잘 지내시죠? 오랜만입...', '(광고) AI로 주가를 예측할 수 있다!']\n",
      "[1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "x = df['메일 본문']\n",
    "y = df['스팸 메일 유무']\n",
    "print(x,y)\n",
    "print('----------------------------------------')\n",
    "print(x.to_list())    #to_list사용하면 데이터프레임형태가 아닌 각 속성값들을 ,를 두고 리스트 안으로\n",
    "print(y.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db187e-a432-4198-a8f2-3ad40b59ec33",
   "metadata": {},
   "source": [
    "###  numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d2e6709c-c206-47c6-9ac4-8ddcd5638be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n"
     ]
    }
   ],
   "source": [
    "np_array = np.arange(0,16).reshape((4,4))\n",
    "\n",
    "print(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f96b6d72-0131-4dff-82a8-cfce7129e5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 데이터 :\n",
      "[[ 0  1  2]\n",
      " [ 4  5  6]\n",
      " [ 8  9 10]\n",
      " [12 13 14]]\n",
      "y 데이터 : [ 3  7 11 15]\n"
     ]
    }
   ],
   "source": [
    "# 앞에 3개 컬럼을 x, 뒤의 마지막 하나를 y\n",
    "X = np_array[:, :3]\n",
    "y = np_array[:,3]\n",
    "\n",
    "print('X 데이터 :')\n",
    "print(X)\n",
    "print('y 데이터 :',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52368cbc-90c7-4a5d-a691-d97427271136",
   "metadata": {},
   "source": [
    "## 6-2. 각각의 x,y에서 train과 test분리-random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ae05866e-42cb-472e-92a3-ed0df534afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "603f4004-4869-47c5-8408-0d0fc9839d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y넣고 test_size=0.2는 전체의 20%가 테스트용, random_state=1234는 set.seed(1234)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1131199a-2457-4290-9fce-f9868e8c8efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 4,  5,  6],\n",
       "        [ 8,  9, 10],\n",
       "        [12, 13, 14]]),\n",
       " '---------------------------',\n",
       " array([[0, 1, 2]]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,'---------------------------',X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b2ff459d-627e-458d-ac69-d4f7f684a92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7, 11, 15]), '////', array([3]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train,'////',y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1808e2ee-0742-47d7-92fe-b855a2b4091c",
   "metadata": {},
   "source": [
    "### 다른 random state로하면 랜덤하게 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3259c8af-cfdb-4a60-96e6-979899d5fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4553bf54-27bd-441b-b0e6-a365eec7f2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 8,  9, 10],\n",
       "        [ 0,  1,  2],\n",
       "        [ 4,  5,  6]]),\n",
       " '---------------------------',\n",
       " array([[12, 13, 14]]))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,'---------------------------',X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dd3dfda1-2269-49bd-a890-dd520d9b0b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([11,  3,  7]), '////', array([15]))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train,'////',y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
